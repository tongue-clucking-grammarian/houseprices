{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting House Prices with Regression","metadata":{}},{"cell_type":"markdown","source":"This dataset comes from a Kaggle competition and describes 1460 residences in Ames, Iowa based on 79 factors such as size, age and quality of the property. The goal is to develop a model to predict price of the house. Let's also test the notion that house prices have been high lately.","metadata":{}},{"cell_type":"code","source":"# Load Packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\n\n# List data paths \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Reading data \ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-08T21:29:50.934935Z","iopub.execute_input":"2022-09-08T21:29:50.936023Z","iopub.status.idle":"2022-09-08T21:29:50.983357Z","shell.execute_reply.started":"2022-09-08T21:29:50.935964Z","shell.execute_reply":"2022-09-08T21:29:50.982134Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Initial data analysis and tidying\nThe first step is to understand our dataset better. I take a look at the data and notice factors like Fireplace Quality and Fence Quality have a lot of missing data. For simplicity, I decided to remove all columns that contain any missing data, reducing the number of factors from 79 to 61. \nI feel this is a poor practice since some factors such as Electrical System are only missing a few values, it'd be better to throw out the few missing values than throw out the entire column. ","metadata":{}},{"cell_type":"code","source":"# Taking a look \ntrain.head(5)\n# Are there any gaps in the data?\nprint(train.info())\n# Yes Alley, PoolQC, Fence, MiscFeature, and FireplaceQu are missing a lot of data\n\n# Remove all missing values\ntrain = train.dropna(axis='columns',how='any')","metadata":{"execution":{"iopub.status.busy":"2022-09-08T21:48:29.888391Z","iopub.execute_input":"2022-09-08T21:48:29.888815Z","iopub.status.idle":"2022-09-08T21:48:29.937850Z","shell.execute_reply.started":"2022-09-08T21:48:29.888778Z","shell.execute_reply":"2022-09-08T21:48:29.937092Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Correlation heatmap with a triangular mask\nplt.figure(figsize=(30,20))\nsns.heatmap(train.corr(),mask=np.triu(np.ones_like(train.corr())),cmap=\"BuPu\")","metadata":{"execution":{"iopub.status.busy":"2022-09-08T21:16:10.070585Z","iopub.execute_input":"2022-09-08T21:16:10.070993Z","iopub.status.idle":"2022-09-08T21:16:11.251002Z","shell.execute_reply.started":"2022-09-08T21:16:10.070958Z","shell.execute_reply":"2022-09-08T21:16:11.250076Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model 1: Decision Tree with Correlated Factors\nNow that we've visualized and cleaned the data, let's build our first model to predict price. This model will be a decision tree using a few of the most highly correlated factors such as Overall Quality, Year Built and factors relating to Area of the property (how big it is).","metadata":{}},{"cell_type":"code","source":"y = train.SalePrice # Setting the target variable\nfeatures = ['OverallQual','YearBuilt','GrLivArea','GarageCars','GarageArea']\nX = train[features]\ntrain_X, test_X, train_y, test_y = train_test_split(X,y,random_state=1) \nmodel1 = DecisionTreeRegressor(random_state=1)\nmodel1.fit(train_X,train_y)\n\npredictions = model1.predict(test_X)\nprint(mean_absolute_error(test_y,predictions))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T23:02:11.676818Z","iopub.execute_input":"2022-09-08T23:02:11.677224Z","iopub.status.idle":"2022-09-08T23:02:11.696879Z","shell.execute_reply.started":"2022-09-08T23:02:11.677193Z","shell.execute_reply":"2022-09-08T23:02:11.695266Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"On average our model is off by $27,404. Let's see if we can do better just by adding more factors.","metadata":{}},{"cell_type":"markdown","source":"## Model 2: Add More Variables and Nodes","metadata":{}},{"cell_type":"code","source":"y = train.SalePrice # Setting the target variable\nfeatures = ['MSSubClass','LotArea','OverallQual','OverallCond','YearBuilt',\n           'YearRemodAdd','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n           '1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath',\n           'FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces',\n           'GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch',\n           'ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']\nX = train[features]\ntrain_X, test_X, train_y, test_y = train_test_split(X,y,random_state=1) \nmodel2 = DecisionTreeRegressor(random_state=1)\nmodel2.fit(train_X,train_y)\n\npredictions = model2.predict(test_X)\nprint(mean_absolute_error(test_y,predictions))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T23:18:55.181389Z","iopub.execute_input":"2022-09-08T23:18:55.181763Z","iopub.status.idle":"2022-09-08T23:18:55.216467Z","shell.execute_reply.started":"2022-09-08T23:18:55.181732Z","shell.execute_reply":"2022-09-08T23:18:55.215336Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"Our model has improved but only slightly by about $2444. This can be refined further by adjusting the max_leaf_nodes input in the DecisionTreeRegressor function. ","metadata":{}},{"cell_type":"code","source":"# Define a function which computes the MAE for comparison of models with different numbers\n# of nodes\ndef get_mae(max_leaf_nodes,train_X,val_X,train_y,val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=1)\n    model.fit(train_X,train_y)\n    preds_val=model.predict(val_X)\n    mae = mean_absolute_error(val_y,preds_val)\n    return(mae)\n\n# Compute the MAE for each number in the array, representing number of nodes\nnode_array = [10,20,30,40,50,60,70,80,90,100]\nfor max_leaf_nodes in node_array:\n    my_mae = get_mae(max_leaf_nodes,train_X,test_X,train_y,test_y)\n    print((max_leaf_nodes,my_mae))\n    \n\n# Or loop to find the best number in a given array\nscores = {leaf_size: get_mae(leaf_size, train_X, test_X, train_y, test_y) \n          for leaf_size in node_array}\nbest_tree_size = min(scores,key=scores.get)\nprint(best_tree_size)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T23:18:59.912902Z","iopub.execute_input":"2022-09-08T23:18:59.913594Z","iopub.status.idle":"2022-09-08T23:19:00.170852Z","shell.execute_reply.started":"2022-09-08T23:18:59.913555Z","shell.execute_reply":"2022-09-08T23:19:00.169767Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"model2 = DecisionTreeRegressor(random_state=1,max_leaf_nodes=90)\nmodel2.fit(train_X,train_y)\n\npredictions = model2.predict(test_X)\nprint(mean_absolute_error(test_y,predictions))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T23:19:02.473742Z","iopub.execute_input":"2022-09-08T23:19:02.474927Z","iopub.status.idle":"2022-09-08T23:19:02.498504Z","shell.execute_reply.started":"2022-09-08T23:19:02.474872Z","shell.execute_reply":"2022-09-08T23:19:02.497561Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"Using 90 leaves or nodes in the Decision Tree gives us a MAE of $23,186. Not bad for such a simple model I suppose, but if we want to be more accurate we'll have to be more clever. Let's try a more sophisticated model: a Random Forest.","metadata":{}},{"cell_type":"markdown","source":"## Model 3: Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest1 = RandomForestRegressor(random_state=1)\nforest1.fit(train_X,train_y)\nfpreds = forest1.predict(test_X)\nprint(mean_absolute_error(test_y,fpreds))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T23:19:24.415706Z","iopub.execute_input":"2022-09-08T23:19:24.416120Z","iopub.status.idle":"2022-09-08T23:19:25.456941Z","shell.execute_reply.started":"2022-09-08T23:19:24.416089Z","shell.execute_reply":"2022-09-08T23:19:25.455587Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nThe Random Forest model is a nice improvement bringing the MAE down to $16,926, but our model is still not very accurate. It's also not very practical because if you're want to predict what your house will sell for, you'll need to input 61 variables. \nI think the model can be improved further by examining the factors more carefully, getting a better idea which factors we can exclude and seeing if any transformations could help","metadata":{}}]}